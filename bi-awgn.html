<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 The binary-input AWGN channel | Transmitting short packets over wireless channels—an information-theoretic perspective</title>
  <meta name="description" content="An information-theoretic perspective on the optimal design of latency-constrained communication systems" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 The binary-input AWGN channel | Transmitting short packets over wireless channels—an information-theoretic perspective" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://gdurisi.github.io/fbl-notes/" />
  <meta property="og:image" content="https://gdurisi.github.io/fbl-notes/images/Section2/figure2_6/logo.png" />
  <meta property="og:description" content="An information-theoretic perspective on the optimal design of latency-constrained communication systems" />
  <meta name="github-repo" content="gdurisi/fbl-notes" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 The binary-input AWGN channel | Transmitting short packets over wireless channels—an information-theoretic perspective" />
  
  <meta name="twitter:description" content="An information-theoretic perspective on the optimal design of latency-constrained communication systems" />
  <meta name="twitter:image" content="https://gdurisi.github.io/fbl-notes/images/Section2/figure2_6/logo.png" />

<meta name="author" content="Giuseppe Durisi and Alejandro Lancho" />


<meta name="date" content="2020-12-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction.html"/>
<link rel="next" href="references.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
Short packets over wireless channels

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#simulation-code"><i class="fa fa-check"></i><b>1.1</b> Simulation code</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#status-of-these-notes"><i class="fa fa-check"></i><b>1.2</b> Status of these notes</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="introduction.html"><a href="introduction.html#the-need-for-a-nonasymptotic-theory-of-packet-transmission"><i class="fa fa-check"></i><b>2.1</b> The need for a nonasymptotic theory of packet transmission</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="bi-awgn.html"><a href="bi-awgn.html"><i class="fa fa-check"></i><b>3</b> The binary-input AWGN channel</a><ul>
<li class="chapter" data-level="3.1" data-path="bi-awgn.html"><a href="bi-awgn.html#eps-rate-tradeoff"><i class="fa fa-check"></i><b>3.1</b> The tradeoff between transmission rate and packet error probability</a></li>
<li class="chapter" data-level="3.2" data-path="bi-awgn.html"><a href="bi-awgn.html#an-achievability-bound"><i class="fa fa-check"></i><b>3.2</b> An achievability bound</a><ul>
<li class="chapter" data-level="3.2.1" data-path="bi-awgn.html"><a href="bi-awgn.html#the-random-coding-union-bound-with-parameter-s"><i class="fa fa-check"></i><b>3.2.1</b> The random-coding union bound with parameter s</a></li>
<li class="chapter" data-level="3.2.2" data-path="bi-awgn.html"><a href="bi-awgn.html#evaluation-of-the-rcus-for-the-bi-awgn-channel"><i class="fa fa-check"></i><b>3.2.2</b> Evaluation of the RCUs for the bi-AWGN channel</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="bi-awgn.html"><a href="bi-awgn.html#a-converse-bound"><i class="fa fa-check"></i><b>3.3</b> A converse bound</a><ul>
<li class="chapter" data-level="3.3.1" data-path="bi-awgn.html"><a href="bi-awgn.html#binary-hypothesis-testing"><i class="fa fa-check"></i><b>3.3.1</b> Binary hypothesis testing</a></li>
<li class="chapter" data-level="3.3.2" data-path="bi-awgn.html"><a href="bi-awgn.html#the-metaconverse-bound"><i class="fa fa-check"></i><b>3.3.2</b> The metaconverse bound</a></li>
<li class="chapter" data-level="3.3.3" data-path="bi-awgn.html"><a href="bi-awgn.html#mc-biawgn"><i class="fa fa-check"></i><b>3.3.3</b> Evaluation of the metaconverse bound for the bi-AWGN channel</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="bi-awgn.html"><a href="bi-awgn.html#a-numerical-example"><i class="fa fa-check"></i><b>3.4</b> A numerical example</a></li>
<li class="chapter" data-level="3.5" data-path="bi-awgn.html"><a href="bi-awgn.html#the-normal-approximation"><i class="fa fa-check"></i><b>3.5</b> The normal approximation</a><ul>
<li class="chapter" data-level="3.5.1" data-path="bi-awgn.html"><a href="bi-awgn.html#a-normal-approximation-for-the-bi-awgn-channel"><i class="fa fa-check"></i><b>3.5.1</b> A normal approximation for the bi-AWGN channel</a></li>
<li class="chapter" data-level="3.5.2" data-path="bi-awgn.html"><a href="bi-awgn.html#the-accuracy-of-the-normal-approximation"><i class="fa fa-check"></i><b>3.5.2</b> The accuracy of the normal approximation</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="bi-awgn.html"><a href="bi-awgn.html#performance-of-practical-error-correcting-codes"><i class="fa fa-check"></i><b>3.6</b> Performance of practical error-correcting codes</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Transmitting short packets over wireless channels—an information-theoretic perspective</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bi-awgn" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> The binary-input AWGN channel</h1>
<div id="eps-rate-tradeoff" class="section level2">
<h2><span class="header-section-number">3.1</span> The tradeoff between transmission rate and packet error probability</h2>
<p>We will start our tour through finite-blocklength information theory by analyzing one of the simplest, yet practically relevant, point-to-point communication channel: <strong>the discrete-time binary-input AWGN channel</strong>.
The input-output relation of such a channel is
<span class="math display" id="eq:io">\[\begin{equation}
y_j=\sqrt{\rho}x_j + w_j, \quad j=1,\dots, n. \tag{3.1}
\end{equation}\]</span>
Here, <span class="math inline">\(x_k\)</span> is the channel input, which we assume to belong to the set <span class="math inline">\(\{-1,1\}\)</span> whereas <span class="math inline">\(\{w_j\}\)</span> is the discrete-time AWGN process. We assume it to be stationary memoryless and that each of its samples has zero mean and unit variance. Hence, <span class="math inline">\(\rho\)</span> can be interpreted as the signal-to-noise ratio (SNR). Throughout, we shall assume that, to transmit an information message, i.e., a set of <span class="math inline">\(k\)</span> information bits, we are allowed to use the bi-AWGN channel <span class="math inline">\(n\)</span> times.</p>
<p>Note that <span class="math inline">\(n\)</span> is closely related to the communication latency. Indeed, assume that to carry each binary symbol <span class="math inline">\(x_k\)</span> we use a continuous-time signal of bandwidth <span class="math inline">\(W\)</span> Hz. Then using the channel <span class="math inline">\(n\)</span> times to transmit our message of <span class="math inline">\(k\)</span> bits entails a delay of <span class="math inline">\(n/W\)</span> seconds. The actual overall delay may be larger if we also include queuing, propagation, and decoding delays. But in a first approximation, we will neglect these additional sources of delay and take <span class="math inline">\(n\)</span> as a proxy for latency. This is not a bad assumption for short-distance communications. In 4G for example, the smallest packet size has a duration of 0.5 milliseconds, whereas the propagation delay to cover a distance of 180 meters is just 30 microseconds.</p>
<p>So if we are interested in low latency, we should choose <span class="math inline">\(n\)</span> as small as possible. But there is a tradeoff between latency and reliability. Indeed, it is intuitively clear that the larger the number of channel uses <span class="math inline">\(n\)</span> we are allowed to occupy for the transmissions of our <span class="math inline">\(k\)</span> bits is, the more reliable the transmission will be. Let us then denote by <span class="math inline">\(\epsilon\)</span> the error probability. We are interested in the following question.</p>
<hr />
<p><strong>Fundamental question</strong>:<br />
What is the smaller <span class="math inline">\(n\)</span> for which the <span class="math inline">\(k\)</span> information bits can be decoded correctly with probability <span class="math inline">\(1-\epsilon\)</span> or larger?</p>
<hr />
<p>Equivalently, we may try to find the largest number of bits <span class="math inline">\(k\)</span> that we can transmit with reliability <span class="math inline">\(1-\epsilon\)</span> or larger using the channel <span class="math inline">\(n\)</span> times, or the smallest probability of error <span class="math inline">\(\epsilon\)</span> that can be achieved for given <span class="math inline">\(k\)</span> and <span class="math inline">\(n\)</span>.</p>
<p>What we will do next is to introduce some notation that will help us characterizing these quantities, and also illustrate why standard asymptotic information theory does not provide satisfactory answers.</p>
<p>How do we transmit the <span class="math inline">\(k\)</span> information bits using <span class="math inline">\(n\)</span> channel uses and how do we recover the <span class="math inline">\(k\)</span> bits from the <span class="math inline">\(n\)</span> received samples <span class="math inline">\(y^n=(y_1,\dots,y_n)\)</span>? We use a channel code, i.e., an encoder that maps the <span class="math inline">\(k\)</span> bits into the <span class="math inline">\(n\)</span> channel uses, and a decoder that provides an estimate of the <span class="math inline">\(k\)</span> information bits from <span class="math inline">\(y^n\)</span>.</p>
<p>Here is a more formal definition:</p>
<!--TODO: Could include a figure of the enc-dec-->

<div class="definition">
<p><span id="def:channel-code-def" class="definition"><strong>Definition 3.1  (Channel code)  </strong></span>A <span class="math inline">\((k,n,\epsilon)\)</span>-code for the bi-AWGN channel with a given SNR <span class="math inline">\(\rho\)</span> consists of</p>
<ul>
<li>an encoder <span class="math inline">\(f:\{1,2,\dots, 2^k\}\mapsto \{-1,1\}^n\)</span> that maps the information message <span class="math inline">\(\jmath \in \{1,2,\dots, 2^k\}\)</span> into <span class="math inline">\(n\)</span> BPSK symbols.
We will refer to the set of all <span class="math inline">\(n\)</span>-dimensional strings produced by the encoder as <em>codebook</em>. Furthermore, we will call each of the string a <em>codeword</em> and <span class="math inline">\(n\)</span> the <em>blocklength</em>.</li>
<li>A decoder <span class="math inline">\(g:\mathbb{R}^n\mapsto \{1,2,\dots, 2^k\}\)</span> that maps the received sequence <span class="math inline">\(y^n \in \mathbb{R}^n\)</span> into a message <span class="math inline">\(\widehat{\jmath}\in \{1,2,\dots, 2^k\}\)</span>, or, possibly, it declares an error.
This decoder satisfies the error-probability constraint
<span class="math inline">\(\mathbb{P}\bigl\{\widehat{\jmath}\neq \jmath\bigr\}\leq \epsilon\)</span>
</div></li>
</ul>
<p>Note that <span class="math inline">\(\epsilon\)</span> should be interpreted as a packet/frame error probability and not as a bit error probability. Indeed, a difference in a single bit between the binary representation of <span class="math inline">\(\jmath\)</span> and <span class="math inline">\(\widehat{\jmath}\)</span> causes an error, according to our definition.</p>
<p>In a nutshell, finite-blocklength information theory allows us to find, for a given SNR, the triplets <span class="math inline">\((k,n,\epsilon)\)</span> for which a
<span class="math inline">\((k,n,\epsilon)\)</span>-code can be found and the triplets <span class="math inline">\((k,n,\epsilon)\)</span> for which no code exists.</p>
<p>Note that doing so is not trivial, since an exhaustive search among all codes is hopeless.
Indeed, even if we fix the decoding rule to, say, maximum likelihood, for a given choice of <span class="math inline">\(k\)</span> and <span class="math inline">\(n\)</span>, one would in principle need to go through <span class="math inline">\(\binom{2^n}{2^k}\)</span> codes.
So for example, if we have <span class="math inline">\(k=5\)</span> and <span class="math inline">\(n=10\)</span>, the number of codes is about <span class="math inline">\(5\times 10^{60}\)</span>.
This means that even if we had a method to evaluate the error probability of a given code in <span class="math inline">\(1\)</span> ms, it would still take <span class="math inline">\(2\times 10^{50}\)</span> years to analyze all possible codes.</p>
<p>Before exploring how and in which sense a characterization of the triplets <span class="math inline">\((k,n,\epsilon)\)</span> can be performed, let us first see what we can do by using the notion of channel capacity introduced by Shannon <span class="citation">[<a href="#ref-shannon48-07a" role="doc-biblioref">3</a>]</span> in his groundbreaking paper, which started the field of information theory.</p>
<p>Let us denote by <span class="math inline">\(k^\star(n,\epsilon)\)</span> the largest number of bits we can transmit using <span class="math inline">\(n\)</span> channel uses with error probability not exceeding <span class="math inline">\(\epsilon\)</span> and by <span class="math inline">\(R^\star(n,\epsilon)=(k^\star(n,\epsilon)/n)\log 2\)</span> the corresponding maximum coding rate, measured in nats per channel use.
Throughout this set of notes, it will be convenient to consistently measure all rates in nats per channel use rather than in bits per channel use, since it will simplify the theoretical derivations.
This is why we multiplied the ratio <span class="math inline">\(k^\star(n,\epsilon)/n\)</span> by <span class="math inline">\(\log 2\)</span>.
In the numerical examples, we will however converts rates in the more familiar unit bit per channel use, by dividing <span class="math inline">\(R\)</span> by <span class="math inline">\(\log 2\)</span>.</p>
<p>Also for a given rate <span class="math inline">\(R=(k/n) \log 2\)</span>, let us denote by <span class="math inline">\(\epsilon^\star(n,R)\)</span> the smallest error probability achievable using <span class="math inline">\(n\)</span> channel uses and a code of rate <span class="math inline">\(R\)</span>.</p>
<p>Since we cannot go through all possible codes, none of these quantities can be actually computed for the parameter values that are of interest in typical wireless communication scenarios.
Actually, even for a given randomly chosen code, evaluating the error probability under the optimal maximum likelihood decoding is in general unfeasible.
Shannon’s capacity provides an <strong>asymptotic characterization</strong> of these quantities.</p>
<p>Specifically, Shannon’s capacity <span class="math inline">\(C\)</span> is the largest communication rate at which we can transfer information messages over a communication channel (in our case, the binary-input AWGN channel given in <a href="bi-awgn.html#eq:io">(3.1)</a>) with arbitrarily small error probability for sufficiently large blocklength.</p>
<p>Mathematically,</p>
<p><span class="math display">\[\begin{equation}
	C=\lim_{\epsilon \to 0} \lim_{n\to \infty} R^\star(n,\epsilon).
\end{equation}\]</span></p>
<p>Some remarks about this formula are in order:</p>
<ul>
<li><p>One should in general worry about the existence of the limits in the definition of capacity that we have just provided. They do exist for the bi-AWGN channel under analysis here.</p></li>
<li><p>It is important to take the limits in this precise order. If we were to invert the order of the limits, we would get <span class="math inline">\(C=0\)</span> for the bi-AWGN channel <a href="bi-awgn.html#eq:io">(3.1)</a>—a not very insightful result.</p></li>
<li><p>For the channel in <a href="bi-awgn.html#eq:io">(3.1)</a> (and for many other channels of interest in wireless communications), the first limit <span class="math inline">\(\epsilon\to 0\)</span> is actually superfluous. We get the same result for every <span class="math inline">\(\epsilon\)</span> in the interval <span class="math inline">\((0,1)\)</span> as a consequence of the so-called strong converse theorem.
We will show later in this set of notes why this is indeed the case and for which channels this property does not hold.</p></li>
</ul>
<p>One way to explain the relation between capacity and finite-blocklength metrics is through the following figure, which artistically illustrates <span class="math inline">\(\epsilon^\star(n,R)\)</span> as a function for <span class="math inline">\(R\)</span> for various values of <span class="math inline">\(n\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:fblintro"></span>
<img src="images/Section3/figure3_1/fbl_intro.png" alt="Error probability as a function of the rate" width="70%" />
<p class="caption">
Figure 3.1: Error probability as a function of the rate
</p>
</div>
<p>Although we do not know how to compute <span class="math inline">\(\epsilon^\star(n,R)\)</span>, Shannon was able to show that this function converges to a step function centered at <span class="math inline">\(C\)</span> in the limit <span class="math inline">\(n\to\infty\)</span>. This means that when <span class="math inline">\(R&lt;C\)</span>, one can achieve arbitrarily low <span class="math inline">\(\epsilon\)</span> by choosing <span class="math inline">\(n\)</span> sufficiently large. If however <span class="math inline">\(R&gt;C\)</span> the packet error probability goes to <span class="math inline">\(1\)</span> as we increase <span class="math inline">\(n\)</span>.</p>
<p>We know from Shannon’s channel coding theorem that, for stationary memoryless channels like the binary input AWGN channel <a href="bi-awgn.html#eq:io">(3.1)</a>, the channel capacity <span class="math inline">\(C\)</span> is</p>
<p><span class="math display" id="eq:capacity">\[\begin{equation}
 C=\max_{P_X} I(X;Y) \tag{3.2}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(P_X\)</span> denotes the probability distribution on the input <span class="math inline">\(X\)</span> and <span class="math inline">\(I(X;Y)\)</span> is the mutual information</p>
<p><span class="math display" id="eq:mi">\[\begin{equation}
    I(X;Y)= \mathbb{E}\left[\log\frac{P_{Y|X}(Y|X)}{P_Y(Y)}\right].
    \tag{3.3}
\end{equation}\]</span></p>
<p>Here, the expectation is with respect to the joint distribution of <span class="math inline">\((X,Y)\)</span>. Furthermore, <span class="math inline">\(P_{Y|X}\)</span> denotes the channel law</p>
<p><span class="math display" id="eq:transition-prob-bi-AWGN">\[\begin{equation}
  P_{Y | X} (y |x)=\frac{1}{\sqrt{2\pi}}\exp\left(- \frac{(y-\sqrt{\rho}x)^2}{2} \right)
  \tag{3.4}
\end{equation}\]</span></p>
<p>and <span class="math inline">\(P_Y\)</span> is the distribution on the channel output <span class="math inline">\(Y\)</span> induced by <span class="math inline">\(P_X\)</span> through <span class="math inline">\(P_{Y| X}\)</span>.
Specifically, <span class="math inline">\(P_Y(y)=\mathbb{E}\left[P_{Y|X}(y|\bar{X})\right]\)</span> where <span class="math inline">\(\bar{X}\sim P_X\)</span> and independent of <span class="math inline">\(X\)</span>.</p>
<p>A remark on notation is in order. It will be convenient from now on to distinguish between deterministic and random quantities. We will use uppercase letters such as <span class="math inline">\(X\)</span> to denote random quantities, and lowercase letters such as <span class="math inline">\(x\)</span> to denote their realizations.</p>
<p>It will also be convenient to give a name to the log-likelihood ratio in the argument of the expectation that defines mutual information in <a href="bi-awgn.html#eq:mi">(3.3)</a>.
It is usually called <strong>information density</strong> and denoted by <span class="math inline">\(\imath(x,y)\)</span>.
Hence, <span class="math inline">\(I(X;Y)=\mathbb{E}[\imath(X,Y)]\)</span>.</p>
<p>For the bi-AWGN channel, one can show that the uniform distribution over <span class="math inline">\(\{-1,1\}\)</span> achieves the maximum in <a href="bi-awgn.html#eq:capacity">(3.2)</a>.
The induced output distribution <span class="math inline">\(P_Y\)</span> is a Gaussian mixture <span class="math display">\[P_Y= \frac{1}{2}\mathcal{N}(-\sqrt{\rho},1)+\frac{1}{2}\mathcal{N}(\sqrt{\rho},1)\]</span> and the information density can be readily computed as
<span class="math display" id="eq:info-density">\[\begin{equation}
  \imath(x,y) = \log 2 - \log\left(1+ \exp(-2xy\sqrt{\rho})\right)
  \tag{3.5}
\end{equation}\]</span></p>
<p>Finally, the channel capacity <span class="math inline">\(C\)</span> (measured in nats per channel use) is given by</p>
<p><span class="math display" id="eq:capacity-bi-awgn">\[\begin{equation}
  C=\frac{1}{\sqrt{2\pi}}\int e^{-z^2/2} \Bigl(\log 2-\log\bigl(1+e^{-2\rho-2z\sqrt{\rho}}\bigr) \Bigr)\,
  \mathrm{d}z. \tag{3.6}
\end{equation}\]</span></p>
<p>Computing capacity takes only few lines of matlab code</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode matlab"><code class="sourceCode matlab"><span id="cb1-1"><a href="bi-awgn.html#cb1-1"></a>f = @(z) exp(-z.^<span class="fl">2</span>/<span class="fl">2</span>)/sqrt(<span class="fl">2</span>*pi) .* (log2(<span class="fl">2</span>) - log2(<span class="fl">1</span>+exp(-<span class="fl">2</span>*rho-<span class="fl">2</span>*sqrt(rho)*z)));</span>
<span id="cb1-2"><a href="bi-awgn.html#cb1-2"></a>Zmin = -<span class="fl">9</span>; Zmax = <span class="fl">9</span>;</span>
<span id="cb1-3"><a href="bi-awgn.html#cb1-3"></a>C = quadl(f, Zmin, Zmax);</span></code></pre></div>
<p>This code is part of the routine <code>biawgn_stats.m</code> available as part of the <a href="https://github.com/yp-mit/spectre">SPECTRE</a> toolbox. We will talk about this toolbox in later chapters; nevertheless, it may be good to have a look at it already now.</p>
<p>To summarize, the capacity of the bi-AWGN channel is relatively easy to compute, but gives us only the following asymptotic answer to the question about which triplets <span class="math inline">\((k,n,\epsilon)\)</span> are achievable:</p>
<hr />
<p><strong>Asymptotic wisdom</strong>: All triplets <span class="math inline">\((nR/\log 2,n,\epsilon)\)</span> are achievable for sufficiently large <span class="math inline">\(n\)</span>, provided that <span class="math inline">\(R&lt;C\)</span>.</p>
<hr />
<p>Although asymptotic, the characterization of the achievable triplets provided by Shannon’s capacity has turned out to be very important! Not only it provides a benchmark against which to compare the performance of actual codes.
It also provides a useful abstraction of the physical layer (especially when capacity is known in closed form), which can be used for the design of higher-layer protocols, such as resource allocation and user scheduling algorithms.</p>
<p>Unfortunately, the asymptotic answer provided by Shannon’s capacity turns out to be loose when the blocklength <span class="math inline">\(n\)</span> is small.
As we shall see, finite-blocklength information theory will give us a much more precise characterization.</p>
<p>Let us go back to the tradeoff between <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(R\)</span> depicted artistically in Fig. <a href="bi-awgn.html#fig:fblintro">3.1</a>.
In Fig. <a href="bi-awgn.html#fig:fblawgnevr">3.2</a> below, we show how finite-blocklength information theory can be used to provide a very accurate quantitative characterization of this trade-off for a fixed blocklength <span class="math inline">\(n\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:fblawgnevr"></span>
<img src="images/Section3/figure3_2/eps_vs_rate.png" alt="Finite-blocklength upper and lower bounds on $\epsilon^*(n,R)$ as a function of the rate $R$ measured in bits per channel use for different values of the blocklength $n$" width="70%" />
<p class="caption">
Figure 3.2: Finite-blocklength upper and lower bounds on <span class="math inline">\(\epsilon^*(n,R)\)</span> as a function of the rate <span class="math inline">\(R\)</span> measured in bits per channel use for different values of the blocklength <span class="math inline">\(n\)</span>
</p>
</div>
<p>In the figure we have set <span class="math inline">\(\rho=0.189\)</span> dB. For this value of SNR, the Shannon capacity is equal to <span class="math inline">\(0.5\)</span> bits per channel use.
As shown by the dashed vertical red line in the figure, this implies that for all rates below <span class="math inline">\(0.5\)</span> bits per channel use an arbitrary low error probability can be achieved by letting the blocklength <span class="math inline">\(n\to\infty\)</span>.
The other curves in the figure illustrate <span style="color:blue">upper (achievability) bounds</span> in blue and <span style="color:red">lower (converse) bounds</span> on the error probability <span class="math inline">\(\epsilon^*(n,R)\)</span> for different values of the blocklength <span class="math inline">\(n\)</span>.
The converse bounds give us an impossibility result: for a given blocklength, no code with parameters <span class="math inline">\(\{R,\epsilon(R,n)\}\)</span> that lie below the red curve can be found.
The achievability result instead demonstrates the existence of codes with parameters <span class="math inline">\(\{R,\epsilon(R,n)\}\)</span> above the blue curve (although, it does not indicate how to construct them explicitly).
Between the two curves lies a (small) region for which we cannot prove or disprove the existence of a code.</p>
<p>Here is how we can use this figure for system design.
If, for example, we want codes operating at an error probability of <span class="math inline">\(\epsilon=10^{-8}\)</span>, Fig. <a href="bi-awgn.html#fig:fblawgnevr">3.2</a> tells us that for a blocklength of <span class="math inline">\(128\)</span> channel uses, we should use a code of rate below <span class="math inline">\(0.19\)</span> bits/channel use.
However, if we can afford a blocklength of <span class="math inline">\(512\)</span> channel uses, codes with rate as large as <span class="math inline">\(0.31\)</span> bits per channel uses can be used.
Both values are significantly smaller than the <span class="math inline">\(0.5\)</span> bits per channel use promised by the Shannon capacity.</p>
</div>
<div id="an-achievability-bound" class="section level2">
<h2><span class="header-section-number">3.2</span> An achievability bound</h2>
<div id="the-random-coding-union-bound-with-parameter-s" class="section level3">
<h3><span class="header-section-number">3.2.1</span> The random-coding union bound with parameter s</h3>
<p>As already mentioned, achievability bounds in finite-blocklength information theory give us an upper bound on <span class="math inline">\(\epsilon^*(k,n)\)</span>, or equivalently, a lower bound on <span class="math inline">\(k^\star(n,\epsilon)\)</span>, and, hence on <span class="math inline">\(R^\star(k,n)\)</span> for a given SNR value.</p>
<p>Many such bounds are available in the literature: some are based on threshold decoding, such as the dependence-testing bound <span class="citation">[<a href="#ref-polyanskiy10-05a" role="doc-biblioref">1</a>]</span>. Some are based on hypothesis testing such as the <span class="math inline">\(\kappa\beta\)</span> bound <span class="citation">[<a href="#ref-polyanskiy10-05a" role="doc-biblioref">1</a>]</span> and the <span class="math inline">\(\beta\beta\)</span> bound <span class="citation">[<a href="#ref-yang18-09a" role="doc-biblioref">4</a>]</span>.
Here, we will review a variation of the random-coding union (RCU) bound in <span class="citation">[<a href="#ref-polyanskiy10-05a" role="doc-biblioref">1</a>]</span> introduced in <span class="citation">[<a href="#ref-martinez11-02a" role="doc-biblioref">5</a>]</span>, which is commonly referred to as RCU bound with parameter <span class="math inline">\(s\)</span> (RCUs).</p>
<p>This bound is appealing because</p>
<ul>
<li>it is tight for a large range of error-probability values</li>
<li>it is reasonably easy to evaluate numerically</li>
<li>it lends itself to an easy generalization to arbitrary, mismatch decoding rules. This is important, because it enables its applications in many setups of practical relevance, including when one uses pilot symbols to estimate the channel in a fading environment, and nearest-neighbor detection based on the estimated CSI at the receiver.</li>
</ul>
<p>Like most achievability bounds in information theory, the RCUs bound relies on <em>random coding</em>, a technique introduced by Shannon <span class="citation">[<a href="#ref-shannon48-07a" role="doc-biblioref">3</a>]</span> to establish achievability results.
The idea is as follows: rather than analyzing the performance of a given code, one analyzes the average performance of a random ensemble of codes, whose codewords are drawn independently from a given distribution. Then, one shows that the average error probability, averaged over this ensemble is upper-bound by a suitable, easy-to-evaluate function of <span class="math inline">\(k\)</span> and <span class="math inline">\(n\)</span>. Finally, one concludes that there must exist at least a code in the ensemble whose error probability is upper-bounded by that function.</p>
<p>We will state for future uses the RCUs bound in a more general setup than the bi-AWGN channel. Specifically, let us denote by <span class="math inline">\(P_{Y^n | X^n}\)</span> a general channel law that describes in probabilistic terms the input-output relation corresponding to the transmission of a block of <span class="math inline">\(n\)</span> symbols over a generic channel.
For example, in the bi-AWGN case, we have</p>
<p><span class="math display">\[P_{Y^n | X^n}(y^n | x^n)=\prod_{j=1}^{n} P_{Y | X}(y_j | x_j)\]</span></p>
<p>where <span class="math inline">\(P_{Y | X}(y |x)\)</span> was given in <a href="bi-awgn.html#eq:transition-prob-bi-AWGN">(3.4)</a>.</p>
<p>We are now ready to state the RCUs bound.</p>

<div class="theorem">
<span id="thm:rcus" class="theorem"><strong>Theorem 3.1  (RCUs bound)  </strong></span>Fix an <span class="math inline">\(s&gt;0\)</span> and an input distribution <span class="math inline">\(P_{X^n}\)</span>. Then for every <span class="math inline">\(k\)</span> and <span class="math inline">\(n\)</span>, there exists a <span class="math inline">\((k,n,\epsilon)\)</span>-code whose error probability is upper-bounded as
<span class="math display" id="eq:rcus-bound">\[\begin{equation}
  \epsilon \leq \mathbb{E}\left[ \exp\left[-\max\left\{0, \imath_s(X^n,Y^n)-\log(2^k-1) \right\} \right] \right].
  \tag{3.7}
\end{equation}\]</span>
Here, <span class="math inline">\((X^n,Y^n)\)</span> is distributed as <span class="math inline">\(P_{X^n}P_{Y^n | X^n}\)</span> and <span class="math inline">\(\imath_s(x^n,y^n)\)</span> is the generalized information density, which is defined as
<span class="math display" id="eq:generalized-info-dens">\[\begin{equation}
  \imath_s(x^n,y^n)=\log \frac{P^s_{Y^n | X^n} (y^n | x^n)}{\mathbb{E}\left[P^s_{Y^n | X^n}(y^n | \bar{X}^n)\right]}
  \tag{3.8}
\end{equation}\]</span>
with <span class="math inline">\(\bar{X}^n\)</span> distributed as <span class="math inline">\(P_{X^n}\)</span> and independent of <span class="math inline">\(X^n\)</span>.
</div>

<p>Note that the generalized information density in <a href="bi-awgn.html#eq:generalized-info-dens">(3.8)</a> is closely related to the information density in <a href="bi-awgn.html#eq:info-density">(3.5)</a> and the two quantities coincide for the case <span class="math inline">\(s=n=1\)</span>.
We will next prove <a href="bi-awgn.html#eq:rcus-bound">(3.7)</a>. Implementation details for the case of the bi-AWGN channel, including a matlab script, are given at the end of this section.</p>

<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> 
Let <span class="math inline">\(M=2^k\)</span> be the number of messages associated with the <span class="math inline">\(k\)</span> information bits. We assign to each message an <span class="math inline">\(n\)</span>-dimensional codeword <span class="math inline">\(C_j\)</span>, <span class="math inline">\(j=1,\dots,M\)</span> generated independently from an <span class="math inline">\(n\)</span>-dimensional distribution <span class="math inline">\(P_{X^n}\)</span>.
We shall analyze the average error probability under maximum-likelihood decoding, averaged with respect to all possible codewords generated in this way.
The reader familiar with the derivation of Gallager random coding error exponent <span class="citation">[<a href="#ref-gallager68a" role="doc-biblioref">6</a>]</span> will recognize most of the following steps.
For a given realization <span class="math inline">\(c_1,\dots,c_M\)</span> of our random codebook, the error probability can be upper-bounded as</p>
<p><span class="math display">\[\epsilon(c_1,\dots,c_M) \leq \frac{1}{M} \sum_{j=1}^M \mathbb{P}\left[ \bigcup_{t=1, t\neq j}^M \left\{ P_{Y^n | X^{n}}(Y^n | c_t)\geq P_{Y^n | X^{n}}(Y^n | c_j)  \right\} \right]. \]</span></p>
<p>In words, under maximum-likelihood decoding, an error occurs whenever the transmitted codeword has a smaller likelihood than one of the other codewords. The inequality here comes from the fact that we assumed that all ties produce errors, which is pessimistic.
We now average over all codebooks. Since all codewords are identically distributed, we can assume without loss of generality that the transmitted codeword is the first one. Hence,</p>
<p><span class="math display">\[\mathbb{E}\left[\epsilon(C_1,\dots,C_M)\right]\leq \mathbb{P}\left[ \bigcup_{t=2}^M  \left\{ P_{Y^n | X^{n}}(Y^n | C_t)\geq P_{Y^n | X^{n}}(Y^n | C_1)  \right\} \right]. \]</span></p>
<p>The plan now is to replace the probability of the union of events by a sum of probabilities of individual events by means of the union bound. However, to make sure that the resulting bound is not too loose, we operate as follows. We first condition on <span class="math inline">\(C_1\)</span> and <span class="math inline">\(Y^n\)</span>. Then, we apply the union bound with respect to <span class="math inline">\(C_2,\dots,C_M\)</span>, and finally we average over <span class="math inline">\(C_1\)</span> and <span class="math inline">\(Y^n\)</span>. These steps yield</p>
<p><span class="math display">\[\mathbb{E}\left[\epsilon(C_1,\dots,C_M)\right]\leq \mathbb{E}_{C_1,Y^n}\left[\min\left\{1,\sum_{t=2}^M\mathbb{P}\left[    P_{Y^n | X^{n}}(Y^n | C_t)\geq P_{Y^n | X^{n}}(Y^n | C_1) | C_1,Y^n  \right]\right\}\right]. \]</span></p>
<p>Here, the <span class="math inline">\(\min\)</span> tightens our results by ensuring that our upper bound on the probability term does not exceed <span class="math inline">\(1\)</span>.
Now, since all codewords <span class="math inline">\(C_2,\dots,C_M\)</span> are identically distributed, the <span class="math inline">\((M-1)\)</span> probability terms are identical. Let us denote by <span class="math inline">\(X^n\)</span> the transmitted codeword and by <span class="math inline">\(\bar{X^n}\)</span> one of the other codewords.
Using the observation above and the notation just introduced, we can rewrite our bound as</p>
<p><span class="math display" id="eq:RCU">\[\begin{equation}
\epsilon \leq \mathbb{E}_{X^n,Y^n}\left[\min\left\{1, (M-1)\mathbb{P}\left[    P_{Y^n|X^n}(Y^n|\bar{X}^n)\geq P_{Y^n|X^n}(Y^n|X^n)| X^n,Y^n  \right]\right\}\right]. \tag{3.9}
\end{equation}\]</span>
This is precisely the RCU bound proposed in <span class="citation">[<a href="#ref-polyanskiy10-05a" role="doc-biblioref">1</a>]</span> .</p>
<p>Unfortunately, this bound is difficult to compute for the bi-AWGN channel. Indeed, no closed-form expression for the probability term in the bound is available. And a naïve Monte-Carlo approach to compute this term is unfeasible (although more sophisticated saddle-point approximation methods could be adopted instead <span class="citation">[<a href="#ref-font-segura18-03a" role="doc-biblioref">7</a>]</span>—we will review them in a later chapter). To see why, assume that <span class="math inline">\(k=100\)</span> so that <span class="math inline">\(M=2^k=2^{100}\)</span>. Then, to compute the RCU bound, we would need to evaluate numerically tail probabilities as small as <span class="math inline">\(2^{-100}\approx 10^{-30}\)</span>.</p>
<p>One way to avoid this is to upper-bound the probability term using the generalized Markov inequality.
This inequality states that for every nonnegative random variable <span class="math inline">\(T\)</span> and for every positive <span class="math inline">\(s\)</span>,</p>
<p><span class="math display">\[ \mathbb{P}[T&gt;\gamma]\leq \frac{\mathbb{E}[T^s]}{\gamma^s}.\]</span></p>
<p>Applying this inequality to the probability term in the RCU bound, for fixed <span class="math inline">\(X^n=x^n\)</span> and <span class="math inline">\(Y^n=y^n\)</span>, we obtain</p>
<p><span class="math display" id="eq:MarkovRCU">\[\begin{equation}
\mathbb{P}\left[  P_{Y^n|X^n}(y^n|\bar{X}^n)\geq P_{Y^n|X^n}(y^n|x^n)\right]\leq \frac{\mathbb{E}\left[ P_{Y^n|X^n}^s(y^n|\bar{X}^n) \right]}{P^s_{Y^n|X^n}(y^n|x^n)}=e^{-\imath_s(x^n;y^n)}. \tag{3.10} 
\end{equation}\]</span></p>
Substituting this inequality into the RCU bound <a href="bi-awgn.html#eq:RCU">(3.9)</a>, we obtain the desired RCUs bound after algebraic manipulations.
</div>

</div>
<div id="evaluation-of-the-rcus-for-the-bi-awgn-channel" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Evaluation of the RCUs for the bi-AWGN channel</h3>
<p>To evaluate the RCUs bound for the bi-AWGN we just need to note that
<span class="math display" id="eq:gen-info-dens-biawgn">\[\begin{equation}
  \imath_s(x^n,y^n)= n\log 2 - \sum_{j=1}^n \log\left(1+e^{-2sx_jy_j\sqrt{\rho}}\right)
  \tag{3.11}
\end{equation}\]</span></p>
<p>Few lines of matlab code suffice (author: <a href="https://www.chalmers.se/en/staff/Pages/johanos.aspx">Johan Östman</a>, Chalmers).</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode matlab"><code class="sourceCode matlab"><span id="cb2-1"><a href="bi-awgn.html#cb2-1"></a>function Pe = rcus_biawgn_fixed_s(R,n,rho,s,num_samples)</span>
<span id="cb2-2"><a href="bi-awgn.html#cb2-2"></a><span class="co">%this function computes rcus for given s</span></span>
<span id="cb2-3"><a href="bi-awgn.html#cb2-3"></a>i_s = info_dens_biawgn(n,rho,s,num_samples);</span>
<span id="cb2-4"><a href="bi-awgn.html#cb2-4"></a>Pe = mean( exp( - max(<span class="fl">0</span>, i_s - log(<span class="fl">2</span>^(n*R)-<span class="fl">1</span>) ) ));</span>
<span id="cb2-5"><a href="bi-awgn.html#cb2-5"></a>end</span>
<span id="cb2-6"><a href="bi-awgn.html#cb2-6"></a></span>
<span id="cb2-7"><a href="bi-awgn.html#cb2-7"></a></span>
<span id="cb2-8"><a href="bi-awgn.html#cb2-8"></a>function i_s = info_dens_biawgn(n,rho,s,num_samples)</span>
<span id="cb2-9"><a href="bi-awgn.html#cb2-9"></a>S = [-sqrt(rho), sqrt(rho)]; <span class="co">%constellation</span></span>
<span id="cb2-10"><a href="bi-awgn.html#cb2-10"></a>W = randn(num_samples, n); <span class="co">%create awgn noise</span></span>
<span id="cb2-11"><a href="bi-awgn.html#cb2-11"></a>X = S(randi([<span class="fl">1</span>,<span class="fl">2</span>], num_samples, n)); <span class="co">%sample uniformly from constellation</span></span>
<span id="cb2-12"><a href="bi-awgn.html#cb2-12"></a>Y = X+W; <span class="co">% add noise</span></span>
<span id="cb2-13"><a href="bi-awgn.html#cb2-13"></a>i_s = sum(log(<span class="fl">2</span>) - (s/<span class="fl">2</span>)*(Y-X).^<span class="fl">2</span> - log(exp(-(s/<span class="fl">2</span>)*(Y-S(<span class="fl">1</span>)).^<span class="fl">2</span>) + exp(-(s/<span class="fl">2</span>)*(Y-S(<span class="fl">2</span>)).^<span class="fl">2</span>)),<span class="fl">2</span>); <span class="co">%compute information density samples</span></span>
<span id="cb2-14"><a href="bi-awgn.html#cb2-14"></a>end</span></code></pre></div>
</div>
</div>
<div id="a-converse-bound" class="section level2">
<h2><span class="header-section-number">3.3</span> A converse bound</h2>
<p>Converse bounds in information theory express an impossibility condition. In our case, a converse result says that no <span class="math inline">\((k,n,\epsilon)\)</span>-code can be found for which the triplets <span class="math inline">\((k,n,\epsilon)\)</span> violate a certain relation.
Converse bounds give a lower bound on the error probability <span class="math inline">\(\epsilon^\star(k,n)\)</span>, or equivalently, an upper bound on <span class="math inline">\(k^\star(n,\epsilon)\)</span> and, hence, on <span class="math inline">\(R^\star(k,n)\)</span>.</p>
<p>We will review in this section a very general technique to obtain converse bounds in finite-blocklength information theory, which relies on the so-called <em>metaconverse theorem</em> <span class="citation">[<a href="#ref-polyanskiy10-05a" role="doc-biblioref">1</a>]</span>.</p>
<div id="binary-hypothesis-testing" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Binary hypothesis testing</h3>
<p>Before introducing the metaconverse theorem, we need to review some basic concepts in binary hypothesis testing.</p>
<p>Binary hypothesis testing deals with the following problem (see figure below). We observe a vector <span class="math inline">\(X^n\)</span> which may have been generated according to a distribution <span class="math inline">\(P_{X^n}\)</span> or according to an alternative distribution <span class="math inline">\(Q_{X^n}\)</span>. Our task is to decide which of the two distributions generated <span class="math inline">\(X^n\)</span>.</p>
<p>To do so, we design a test <span class="math inline">\(Z\)</span> whose input is the vector <span class="math inline">\(X^n\)</span> and whose output is a binary value <span class="math inline">\(\{0,1\}\)</span>. We use the convention that <span class="math inline">\(0\)</span> indicates that the test chooses <span class="math inline">\(P_{X^n}\)</span> and <span class="math inline">\(1\)</span> indicates that the test chooses <span class="math inline">\(Q_{X^n}\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:binhyptest"></span>
<img src="images/Section3/hypothesis-testing.png" alt="Binary hypothesis testing" width="70%" />
<p class="caption">
Figure 3.3: Binary hypothesis testing
</p>
</div>
<p>We can think of the test <span class="math inline">\(Z\)</span> as a binary partition of the set of the vectors <span class="math inline">\(X^n\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:binary-partition"></span>
<img src="images/Section3/neyman_pearson.png" alt="Binary hypothesis testing as a binary partition" width="40%" />
<p class="caption">
Figure 3.4: Binary hypothesis testing as a binary partition
</p>
</div>
<p>Actually, we will need the more general notion of <em>randomized test</em>, in which the test is a conditional distribution <span class="math inline">\(P_{Z |X^n}\)</span>. Coarsely speaking, for some values of <span class="math inline">\(X^n\)</span>, we may flip a coin to decide whether <span class="math inline">\(Z=0\)</span> or <span class="math inline">\(Z=1\)</span>.
It turns out that this randomization is needed at the boundary between the two regions.</p>
<p>We are interested in the test that minimizes the probability of misclassification when <span class="math inline">\(X^n\)</span> is generated according to <span class="math inline">\(Q_{X^n}\)</span> given that the probability of correct classification under <span class="math inline">\(P_{X^n}\)</span> is above a given target. Mathematically, we define the probability of misclassification under <span class="math inline">\(Q_{X^n}\)</span> as</p>
<p><span class="math display">\[Q_{X^n}[Z=0]=\sum_{x^n}Q_{X^n}(x^n)P_{Z | X^n}(0 | x^n). \]</span></p>
<p>To parse this expression, recall that <span class="math inline">\(Z=0\)</span> means that the test chooses <span class="math inline">\(P_{X^n}\)</span>, which is the wrong distribution in this case.
Here, we assumed for simplicity that <span class="math inline">\(X^n\)</span> is a discrete random variable. For the continuous case, one needs to replace the sum with an integral. The probability of correct classification under <span class="math inline">\(P_{X^n}\)</span> is similarly given by</p>
<p><span class="math display">\[P_{X^n}[Z=0]=\sum_{x^n}P_{X^n}(x^n)P_{Z | X^n}(0 | x^n).\]</span></p>
<p>We let <span class="math inline">\(\beta_{\alpha}(P_{X^n},Q_{X^n})\)</span> be the <strong>Neyman-Pearson</strong> beta function, which denotes the smallest possible misclassification probability under <span class="math inline">\(Q_{X^n}\)</span> when the probability of correct classification under <span class="math inline">\(P_{X^n}\)</span> is at least <span class="math inline">\(\alpha\)</span>:</p>
<p><span class="math display">\[\beta_{\alpha}(P_{X^n},Q_{X^n})=\inf_{P_{Z| X^n}: P_{X^n}[Z=0]\geq \alpha} Q_{X^n}[Z=0].\]</span></p>
<p>It follows from the <strong>Neyman-Pearson lemma</strong> (see e.g., p. 144 of the following <a href="http://people.lids.mit.edu/yp/homepage/data/itlectures_v5.pdf">lecture notes</a>) that <span class="math inline">\(\beta_{\alpha}(P_{X^n},Q_{X^n})\)</span> is achieved by the test that computes the log-likelihood ratio <span class="math inline">\(\log \frac{P_{X^n}(x^n)}{Q_{X^n}(x^n)}\)</span> and sets <span class="math inline">\(Z=0\)</span> if the log-likelihood is above a threshold <span class="math inline">\(\gamma\)</span>; it sets <span class="math inline">\(Z=1\)</span> otherwise. Here, <span class="math inline">\(\gamma\)</span> is chosen so that</p>
<p><span class="math display">\[P_{X^n}\left[\log \frac{P_{X^n}(x^n)}{Q_{X^n}(x^n)} \geq \gamma \right]=\alpha.\]</span></p>
<p>Note that to satisfy this equality, one needs in general to use a randomized test that flips a suitably biased coin whenever <span class="math inline">\(\log \frac{P_{X^n}(x^n)}{Q_{X^n}(x^n)} = \gamma\)</span>.</p>
<p>One may wonder at this point what binary hypothesis testing has to do with channel coding. Surely, decoding involves a hypothesis test. The test, however, is not binary but <span class="math inline">\(M\)</span>-ary, since it involves <span class="math inline">\(M=2^k\)</span> codewords. We will see later on in this section why such a binary hypothesis test emerges naturally. We provide for the moment the following intuition.</p>
<p>Consider the binary hypothesis testing problem in which one has to decide whether the pair <span class="math inline">\((X^n, Y^n)\)</span> comes from <span class="math inline">\(P_{X^n,Y^n}\)</span> or from <span class="math inline">\(P_{X^n}P_{Y^n}\)</span>. Assume that all distributions are product distributions, i.e.,</p>
<p><span class="math display">\[ P_{X^n,Y^n}(x^n,y^n)=\prod_{j=1}^n P_{X,Y}(x_j,y_j)\]</span></p>
<p>then <strong>Stein’s lemma</strong> (see page 147 of <span class="citation">[<a href="#ref-polyanskiy19-05a" role="doc-biblioref">8</a>]</span>) implies that for all <span class="math inline">\(\alpha \in (0,1)\)</span></p>
<p><span class="math display">\[  \beta_{\alpha}(P_{X^n,Y^n},P_{X^n}P_{Y^n}) = e^{-n\bigl[I(X;Y) + o(1)\bigr]},\quad n\to\infty.\]</span></p>
<p>Here, <span class="math inline">\(o(1)\)</span> stands for terms that go to zero when <span class="math inline">\(n\rightarrow \infty\)</span>.</p>
<p>In words <span class="math inline">\(\beta_{\alpha}(P_{X^n,Y^n},P_{X^n}P_{Y^n})\)</span> decays exponentially fast in <span class="math inline">\(n\)</span>, with an exponent given by the mutual information <span class="math inline">\(I(X;Y)\)</span>. So if mutual information is useful in characterizing the performance of a communication system in the asymptotic limit <span class="math inline">\(n\to\infty\)</span>, it makes sense that the Neyman-Pearson <span class="math inline">\(\beta\)</span> function is useful to describe performance at finite blocklength.</p>
</div>
<div id="the-metaconverse-bound" class="section level3">
<h3><span class="header-section-number">3.3.2</span> The metaconverse bound</h3>
<p>We are now ready to state our converse bound, which we shall refer to as <strong>metaconverse theorem</strong> although this term is used in <span class="citation">[<a href="#ref-polyanskiy10-05a" role="doc-biblioref">1</a>]</span> for a slightly more general result. As for the RCUs bound in Theorem <a href="bi-awgn.html#thm:rcus">3.1</a>, we will first state and prove the bound for a general channel <span class="math inline">\(P_{Y^n|X^n}\)</span>.
Then, we will particularize it to the bi-AWGN channel.</p>

<div class="theorem">
<p><span id="thm:metaconverse" class="theorem"><strong>Theorem 3.2  (metaconverse)  </strong></span>Fix an arbitrary <span class="math inline">\(Q_{Y^n}\)</span>. Every <span class="math inline">\((k,n,\epsilon)\)</span>–code must satisfy</p>
<span class="math display" id="eq:converse">\[\begin{equation}
  2^k\leq \sup_{P_{X^n}}\frac{1}{\beta_{1-\epsilon}(P_{X^n}P_{Y^n|X^n},P_{X^n}Q_{Y^n})}.
  \tag{3.12}
\end{equation}\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> 
Fix a <span class="math inline">\((k,n,\epsilon)\)</span>–code and let <span class="math inline">\(P_{X^n}\)</span> be the distribution induced by the encoder on the transmitted symbol vector <span class="math inline">\(X^n\)</span>.
We now use the code to perform a binary-hypothesis test between <span class="math inline">\(P_{X^n}P_{Y^n |X^n}\)</span> and <span class="math inline">\(P_{X^n}Q_{Y^n}\)</span>. Specifically, for a given pair <span class="math inline">\((X^n,Y^n)\)</span> we determine the corresponding transmitted message <span class="math inline">\(W\)</span> by inverting the encoder given its output <span class="math inline">\(X^n\)</span>. We also find the decoded message <span class="math inline">\(\hat{W}\)</span> by applying the decoder to <span class="math inline">\(Y^n\)</span>.
If <span class="math inline">\(W=\hat{W}\)</span>, we set <span class="math inline">\(Z=0\)</span> and choose <span class="math inline">\(P_{X^n}P_{Y^n |X^n}\)</span>. Otherwise, we set <span class="math inline">\(Z=1\)</span> and choose <span class="math inline">\(P_{X^n}Q_{Y^n}\)</span>.</p>
<p>Since the given code has an error probability no larger than <span class="math inline">\(\epsilon\)</span> when used on the channel <span class="math inline">\(P_{Y^n |X^n}\)</span>, we conclude that</p>
<p><span class="math display">\[P_{X^n}P_{Y^n |X^n}[Z=0]\geq 1-\epsilon. \]</span></p>
<p>The misclassification probability under the alternative hypothesis <span class="math inline">\(P_{X^n}Q_{Y^n}\)</span> is the probability that the decoder produces a correct message when the channel output <span class="math inline">\(Y^n\)</span> is distributed according to <span class="math inline">\(Q_{Y^n}\)</span>, and, hence, is independent of the channel input <span class="math inline">\(X^n\)</span>.
This probability, which is equal to <span class="math inline">\(1/2^k\)</span> is no larger than the misclassification probability of the optimal test <span class="math inline">\(\beta_{1-\epsilon}(P_{X^n}P_{Y^n|X^n},P_{X^n}Q_{Y^n})\)</span>. Mathematically,</p>
<p><span class="math display">\[\beta_{1-\epsilon}(P_{X^n}P_{Y^n|X^n},P_{X^n}Q_{Y^n})\leq P_{X^n}Q_{Y^n}[Z=0]=\frac{1}{2^k}. \]</span></p>
<p>Hence, for the chosen <span class="math inline">\((k,n,\epsilon)\)</span>–code, we have</p>
<p><span class="math display" id="eq:metaconversegivencode">\[\begin{equation}
  2^k\leq \frac{1}{\beta_{1-\epsilon}(P_{X^n}P_{Y^n|X^n},P_{X^n}Q_{Y^n})}.
  \tag{3.13}
\end{equation}\]</span></p>
We finally maximize over <span class="math inline">\(P_{X^n}\)</span> to obtain a bound that holds for all <span class="math inline">\((k,n,\epsilon)\)</span> codes. This concludes the proof.
</div>

<p>How tight is this bound? It turns out that, for a given code with maximum-likelihood decoder, the bound given in
<a href="bi-awgn.html#eq:metaconversegivencode">(3.13)</a> is tight provided that the auxiliary distribution <span class="math inline">\(Q_{Y^n}\)</span> is chosen in an optimal way.</p>
<p>Specifically, set <span class="math inline">\(Q^\star_{Y^n}(y^n)=\frac{1}{\mu}\max_{\bar{x}^n}P_{Y^n|X^n}(y^n|\bar{x}^n)\)</span> where <span class="math inline">\(\mu\)</span> is a normalization constant chosen so that <span class="math inline">\(Q^\star_{Y^n}\)</span> integrates to <span class="math inline">\(1\)</span>.
Note now that, for a given transmitted codeword <span class="math inline">\(x^n\)</span>, the optimum ML decoder finds the correct codeword (under the simplifying assumption of no ties) provided that</p>
<p><span class="math display">\[P_{Y^n|X^n}(y^n|x^n)\geq  \max_{\bar{x}^n}P_{Y^n|X^n}(y^n|\bar{x}^n).\]</span></p>
<p>This is the same as requiring that</p>
<p><span class="math display">\[ \log\frac{P_{Y^n|X^n}(y^n|x^n)}{Q^\star_{Y^n}(y^n)}\geq \log \mu. \]</span></p>
<p>Furthermore, observe that, since the code has probability of error <span class="math inline">\(\epsilon\)</span> under ML decoding, the following must hold:</p>
<p><span class="math display">\[P_{X^n}P_{Y^n|X^n}\left[\log\frac{P_{Y^n|X^n}(y^n|x^n)}{Q^\star_{Y^n}(y^n)}\geq \log \mu\right]=1-\epsilon \]</span></p>
<p>Hence,</p>
<p><span class="math display">\[\beta_{1-\epsilon}(P_{X^n}P_{Y^n|X^n},P_{X^n}Q_{Y^n})=P_{X^n}Q_{Y^n}\left[\log\frac{P_{Y^n|X^n}(y^n|x^n)}{Q^\star_{Y^n}(y^n)}\geq \log \mu\right]=\frac{1}{2^k}.\]</span></p>
</div>
<div id="mc-biawgn" class="section level3">
<h3><span class="header-section-number">3.3.3</span> Evaluation of the metaconverse bound for the bi-AWGN channel</h3>
<p>At first glance, evaluating the metaconverse bound seems hard because of the maximization over <span class="math inline">\(P_{X^n}\)</span> in <a href="bi-awgn.html#eq:converse">(3.12)</a>.
It turns out that, for some channels like the bi-AWGN channel under consideration here, this maximization can be avoided altogether by choosing the auxiliary distribution <span class="math inline">\(Q_{Y^n}\)</span> in a suitable way.</p>
<p>Specifically, let <span class="math display">\[P_Y= \frac{1}{2}\mathcal{N}(-\sqrt{\rho},1)+\frac{1}{2}\mathcal{N}(\sqrt{\rho},1)\]</span> be the capacity achieving output distribution of our binary-input AWGN channel, i.e., the output distribution induced by the capacity-achieving input distribution (which, as already mentioned, is uniform).</p>
<p>We choose <span class="math inline">\(Q_{Y^n}\)</span> so that the resulting vector <span class="math inline">\(Y^n\)</span> has independent entries, all distributed according to <span class="math inline">\(P_Y\)</span>.
The key observation now is that because of symmetry, for every binary channel input <span class="math inline">\(x^n\)</span>
<span class="math display" id="eq:symmetry">\[\begin{equation}
  \beta_{1-\epsilon}(P_{Y^n | X^n=x^n}, Q_{Y^n} )= \beta_{1-\epsilon}(P_{Y^n | X^n=\bar{x}^n}, Q_{Y^n} ) 
  \tag{3.14}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\bar{x}^n\)</span> is the all-one vector. One can then show (see Lemma 29 in <span class="citation">[<a href="#ref-polyanskiy10-05a" role="doc-biblioref">1</a>]</span>) that when <a href="bi-awgn.html#eq:symmetry">(3.14)</a> holds, then</p>
<p><span class="math display">\[\beta_{1-\epsilon}(P_{X^n}P_{Y^n|X^n},P_{X^n}Q_{Y^n})= \beta_{1-\epsilon}(P_{Y^n | X^n=\bar{x}^n}, Q_{Y^n} )\]</span></p>
<p>Note that the right-hand side of this equality does not depend on <span class="math inline">\(P_{X^n}\)</span>.
Hence, for this (generally suboptimal) choice of <span class="math inline">\(Q_{Y^n}\)</span>, the metaconverse bound simplifies to</p>
<p><span class="math display">\[2^k\leq \frac{1}{
\beta_{1-\epsilon}(P_{Y^n | X^n=\bar{x}^n}, Q_{Y^n} )}.\]</span></p>
<p>Equivalently,
<span class="math display" id="eq:metaconverse-biawgn">\[\begin{equation}
  R \leq -\frac{1}{n}\log \biggl( \beta_{1-\epsilon}(P_{Y^n | X^n=\bar{x}^n}, Q_{Y^n} ) \biggr).
  \tag{3.15}
\end{equation}\]</span></p>
<p>Note that the rate here is measured, as usual, in nats per channel use.</p>
<p>Evaluating the beta function directly through the Neyman-Pearson lemma is challenging, because the corresponding tail probabilities are not known in closed form, and one of them needs to evaluated with very high accuracy (we encountered the same problem when we discussed the evaluation of the RCU bound in Theorem <a href="bi-awgn.html#thm:rcus">3.1</a>).
As we shall discuss later, one can use the saddlepoint methods to approximate these tail probabilities accurately <span class="citation">[<a href="#ref-font-segura18-03a" role="doc-biblioref">7</a>]</span>.</p>
<p>A simpler approach is to further relax the bound using the following inequality on the <span class="math inline">\(\beta\)</span> function (see p. 143 of <span class="citation">[<a href="#ref-polyanskiy19-05a" role="doc-biblioref">8</a>]</span>):
<span class="math display" id="eq:verdu-han">\[\begin{equation}
  \beta_{1-\epsilon}(P_{Y^n| X^n=\bar{x}^n},Q_{Y^n}) \geq
        \sup_{\gamma &gt; 0} \left\{e^{-\gamma}\left(1-\epsilon- P_{Y^n | X^n=\bar{x}^n}\left[\log \frac{P_{Y^n| X^n}(Y^n|\bar{x}^n)}{Q_{Y^n}(Y^n)}\geq \gamma\right]\right)\right\}.
  \tag{3.16}
\end{equation}\]</span></p>
<p>The resulting bound, which is a generalized version of Verdú-Han converse bound <span class="citation">[<a href="#ref-verdu94-07a" role="doc-biblioref">9</a>]</span> can be evaluated efficiently.
It is given below: let <span class="math inline">\(\imath(x,y)\)</span> be the information density defined in <a href="bi-awgn.html#eq:info-density">(3.5)</a>.
It then follows from <a href="bi-awgn.html#eq:verdu-han">(3.16)</a> and from our choice of the auxiliary distribution that</p>
<p><span class="math display" id="eq:vh-bound-bi-awgn">\[\begin{equation}
 R \leq \inf_{\gamma &gt;0} \frac{1}{n}\biggl\{ \gamma - \log\biggl( (1-\epsilon) - P_{Y^n | X^n=\bar{x}^n}\biggl[ \sum_{k=1}^n \imath(x_k,y_k)\geq \gamma \biggr] \biggr)  \biggr\}.
  \tag{3.17}
\end{equation}\]</span></p>
<p>Here is a simple matlab implementation of this bound.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode matlab"><code class="sourceCode matlab"><span id="cb3-1"><a href="bi-awgn.html#cb3-1"></a>function rate=vh_metaconverse_rate(snr,n,epsilon,precision)</span>
<span id="cb3-2"><a href="bi-awgn.html#cb3-2"></a>  idvec=compute_samples(snr,n,precision);</span>
<span id="cb3-3"><a href="bi-awgn.html#cb3-3"></a>  idvec=sort(idvec);</span>
<span id="cb3-4"><a href="bi-awgn.html#cb3-4"></a>  ratevec=zeros(length(idvec),<span class="fl">1</span>);</span>
<span id="cb3-5"><a href="bi-awgn.html#cb3-5"></a>  for i=<span class="fl">1</span>:<span class="fl">1</span>:precision,</span>
<span id="cb3-6"><a href="bi-awgn.html#cb3-6"></a>    prob=sum(idvec&lt;idvec(i))/precision;</span>
<span id="cb3-7"><a href="bi-awgn.html#cb3-7"></a>    ratevec(i)=idvec(i)-log(max(prob-epsilon,<span class="fl">0</span>));</span>
<span id="cb3-8"><a href="bi-awgn.html#cb3-8"></a>  end</span>
<span id="cb3-9"><a href="bi-awgn.html#cb3-9"></a>  rate=min(ratevec)/(n*log(<span class="fl">2</span>));</span>
<span id="cb3-10"><a href="bi-awgn.html#cb3-10"></a>end</span>
<span id="cb3-11"><a href="bi-awgn.html#cb3-11"></a></span>
<span id="cb3-12"><a href="bi-awgn.html#cb3-12"></a></span>
<span id="cb3-13"><a href="bi-awgn.html#cb3-13"></a>function id=compute_samples(snr,n,precision)</span>
<span id="cb3-14"><a href="bi-awgn.html#cb3-14"></a>  normrv=randn(n,precision)*sqrt(snr)+snr;</span>
<span id="cb3-15"><a href="bi-awgn.html#cb3-15"></a>  idvec=log(<span class="fl">2</span>)-log(<span class="fl">1</span>+exp(-<span class="fl">2</span>.*normrv));</span>
<span id="cb3-16"><a href="bi-awgn.html#cb3-16"></a>  id=sum(idvec);</span>
<span id="cb3-17"><a href="bi-awgn.html#cb3-17"></a>end</span></code></pre></div>
</div>
</div>
<div id="a-numerical-example" class="section level2">
<h2><span class="header-section-number">3.4</span> A numerical example</h2>
<p>In Figure <a href="bi-awgn.html#fig:epsvssnr">3.5</a> below we depict the upper and lower bound on <span class="math inline">\(\epsilon^*(n,R)\)</span> resulting by using the RCUs bound <a href="bi-awgn.html#eq:rcus-bound">(3.7)</a> (optimized over the parameter <span class="math inline">\(s\)</span>) and the Verdú-Han relaxation of the metaconverse bound <a href="bi-awgn.html#eq:vh-bound-bi-awgn">(3.17)</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:epsvssnr"></span>
<img src="images/Section3/figure3_5/snr_vs_eps.png" alt="Finite-blocklength upper and lower bounds on $\epsilon^*(n,R)$ as a function of the SNR (dB) for $R=1/2$ bit/channel use and $n=128$" width="70%" />
<p class="caption">
Figure 3.5: Finite-blocklength upper and lower bounds on <span class="math inline">\(\epsilon^*(n,R)\)</span> as a function of the SNR (dB) for <span class="math inline">\(R=1/2\)</span> bit/channel use and <span class="math inline">\(n=128\)</span>
</p>
</div>
<!-- <span style="color:red">[TODO (ALEX): Let us have a plot of the two bounds: $R=1/2$, $n=128$, $\epsilon$ as a function of SNR. Also we should illustrate the metaconverse with error-exponent achieving output distribution, computed with saddlepoint, and RCU computed with saddlepoint]</span> -->
<p>We assume that <span class="math inline">\(R=0.5\)</span> bit/channel use and <span class="math inline">\(n=128\)</span>.<br />
The figure shows the two bounds on the error probability as a function of the SNR <span class="math inline">\(\rho\)</span>.
As shown in the figure the two bounds we have just introduced are not the tightest available.
From an achievability bound perspective, one can tighten the RCUs bound by using instead the RCU bound <a href="bi-awgn.html#eq:RCU">(3.9)</a> directly.
Since this bound is difficult to evaluate numerically for low error probabilities, the use of the saddlepoint approximation, which we shall introduce in a later chapter, is indispensable.
Also one can improve on the Verdú-Han relaxation of the metaconverse, by plotting the metaconverse directly (again with the help of a saddlepoint approximation) and by optimizing the chosen auxiliary distribution.
Specifically, the bound in the figure is obtained by considering as auxiliary distribution the so-called <em>error-exponent achieving</em> output distribution.
We will discuss this bound in a later chapter.
<!--TODO: refer to the right chapters --></p>
</div>
<div id="the-normal-approximation" class="section level2">
<h2><span class="header-section-number">3.5</span> The normal approximation</h2>
<p>Both the RCUs bound in Theorem <a href="bi-awgn.html#thm:rcus">3.1</a> and the metaconverse bound in Theorem <a href="bi-awgn.html#thm:metaconverse">3.2</a> are not in closed form and involve the numerical evaluation of <span class="math inline">\(n\)</span>-dimensional integrals, which can be performed, for example, by means of Monte-Carlo simulations.</p>
<p>In this section, we will perform an asymptotic expansion of both bounds, which reveals that, when suitably optimized, the bounds are asymptotically tight.
More precisely, the bounds allow one to characterize the first two terms of the asymptotic expansion of the maximum coding rate <span class="math inline">\(R^*(n,\epsilon)\)</span> achievable on the bi-AWGN channel <a href="bi-awgn.html#eq:io">(3.1)</a> for a fixed <span class="math inline">\(\epsilon\)</span>, as <span class="math inline">\(n\to\infty\)</span>.
As we shall see, this expansion relies on the central limit theorem, and yields an approximation (obtained by neglecting the error terms in the expansion), which is usually referred to as <strong>normal approximation</strong>.</p>
<p>The key observation is that, for a suitable choice of the input distribution in Theorem <a href="bi-awgn.html#thm:rcus">3.1</a> and of the output distribution in Theorem <a href="bi-awgn.html#thm:metaconverse">3.2</a>, both RCUs and metaconverse bounds turn out to involve the computation of the tail distribution of a sum of independent random variables.
We have already seen that this is the case for the Verdú-Han relaxation <a href="bi-awgn.html#eq:vh-bound-bi-awgn">(3.17)</a> of the metaconverse.</p>
<p>We next demonstrate that this is also true for the RCUs bound in Theorem <a href="bi-awgn.html#thm:rcus">3.1</a>.
To do so, we start by noting that, as we have already demonstrated in <a href="bi-awgn.html#eq:gen-info-dens-biawgn">(3.11)</a>, for our choice of the input distribution, the generalized information density can be expressed as</p>
<p><span class="math display">\[ \imath_s(x^n,y^n)= \sum_{j=1}^n \imath_s(x_j,y_j) \]</span>
where
<span class="math display" id="eq:gen-info-density-single-letter">\[\begin{equation}
  \imath_s(x_j,y_j)= \log 2 - \log(1+e^{-2s x_j y_j \sqrt{\rho}}).
  \tag{3.18}
\end{equation}\]</span>
Let now <span class="math inline">\(U\)</span> be a uniform random variable on <span class="math inline">\([0,1]\)</span>.
We shall use that for every nonnegative random variable <span class="math inline">\(Z\)</span>, we have that <span class="math inline">\(\mathbb{E}[\min\{1,Z\}]= \mathbb{P}[Z\geq U]\)</span>
Using this result in <a href="bi-awgn.html#eq:rcus-bound">(3.7)</a>, we obtain after some algebraic manipulations</p>
<p><span class="math display" id="eq:rcus-as-tail">\[\begin{align}
  \mathbb{E}\left[ \exp\left[-\max\left\{0, \imath_s(X^n,Y^n)-\log(2^k-1) \right\} \right] \right]\\
  =\mathbb{P}\left[\sum_{j=1}^n \imath_s(X_j,Y_j) + \log U \leq \log(2^k-1)\right].
  \tag{3.19}
\end{align}\]</span>
This shows that the RCUs bound can indeed be written as the tail probability of a sum of independent random variables.</p>
<div id="a-normal-approximation-for-the-bi-awgn-channel" class="section level3">
<h3><span class="header-section-number">3.5.1</span> A normal approximation for the bi-AWGN channel</h3>
<p>Before we state the normal approximation, we need to introduce some notation.
Recall the information density we introduced in <a href="bi-awgn.html#eq:info-density">(3.5)</a>.
As we have already noticed, the generalized information density <span class="math inline">\(\imath_s(x_j,y_j)\)</span>
in <a href="bi-awgn.html#eq:gen-info-density-single-letter">(3.18)</a> coincides with the information density when <span class="math inline">\(s=1\)</span>, i.e.,
<span class="math display">\[\imath_1(x_j,y_j)=\imath(x_j,y_j). \]</span>
Note also that the random variables <span class="math inline">\(\imath(X_j,Y_j)\)</span> are independent and identically distributed.
Furthermore, as we have already pointed out, for the case in which <span class="math inline">\(X_j\)</span> is drawn according to the capacity-achieving distribution and <span class="math inline">\(Y_j\)</span> is the corresponding channel output, the expectation of <span class="math inline">\(\imath(X_j,Y_j)\)</span> is equal to the channel capacity given in <a href="bi-awgn.html#eq:capacity-bi-awgn">(3.6)</a>.
Finally, let us denote by <span class="math inline">\(V\)</span> the variance of <span class="math inline">\(\imath(X_j,Y_j)\)</span>, computed with respect to the same distribution:
<span class="math display" id="eq:channel-dispersion-bi-awgn">\[\begin{equation}
  V=\mathrm{Var}[\imath(X_j,Y_j)] = \frac{1}{\sqrt{2\pi}}\int e^{-z^2/2} \Bigl(\log 2-\log\bigl(1+e^{-2\rho-2z\sqrt{\rho}}\bigr) \Bigr)^2\,
  \mathrm{d}z - C^2.
  \tag{3.20}
\end{equation}\]</span>
This quantity is usually referred to as <strong>channel dispersion</strong>.
Let also <span class="math inline">\(Q(\cdot)\)</span> denote the Gaussian Q-function
<span class="math display">\[ Q(z) = \frac{1}{\sqrt{2\pi}}\int_z^{\infty} e^{-\frac{x^2}{2}} dx. \]</span>
Finally, we shall need the following order notation: for two functions <span class="math inline">\(f(n)\)</span> and <span class="math inline">\(g(n)\)</span>, the notation <span class="math inline">\(f(n)=\mathcal{O}(g(n))\)</span> means that <span class="math inline">\(\lim\sup_{n\to\infty} |f(n)/g(n)| &lt;\infty\)</span>.
In words, the ratio <span class="math inline">\(|f(n)/g(n)|\)</span> is bounded for sufficiently large <span class="math inline">\(n\)</span>.
We are now ready to state the normal approximation.</p>

<div class="theorem">
<span id="thm:normalapprox" class="theorem"><strong>Theorem 3.3  (Normal approximation)  </strong></span>For the bi-AWGN channel <a href="bi-awgn.html#eq:io">(3.1)</a>, the maximum coding rate <span class="math inline">\(R^*(n,\epsilon)\)</span> satisfies
<span class="math display" id="eq:normal-approx-bi-awgn">\[\begin{equation}
  \tag{3.21}
  R^*(n,\epsilon)= C-\sqrt{\frac{V}{n}}Q^{-1}(\epsilon) + \mathcal{O}\left(\frac{\log n}{n}\right).
\end{equation}\]</span>
</div>

<p>In the next three sections, we present the proof of this result.
In the first section, we will present the Berry-Esseen central limit theorem, a crucial tool to establish <a href="bi-awgn.html#eq:normal-approx-bi-awgn">(3.21)</a>.
Then we will prove that the asymptotic expansions of the achievability bound in Theorem <a href="bi-awgn.html#thm:rcus">3.1</a> and of the converse bound in Theorem <a href="bi-awgn.html#thm:metaconverse">3.2</a> are both equal to the right-hand-side of <a href="bi-awgn.html#eq:normal-approx-bi-awgn">(3.21)</a>.</p>
<div id="proof-the-berry-esseen-central-limit-theorem" class="section level4">
<h4><span class="header-section-number">3.5.1.1</span> Proof: The Berry-Esseen central-limit theorem</h4>
<p>The Berry-Esseen central-limit theorem (see Theorem 2, Chapter XVI.5 in <span class="citation">[<a href="#ref-feller71-a" role="doc-biblioref">10</a>]</span>) provides a characterization of the error incurred when approximating the tail probability of a suitably normalized sum of independent random variables with the tail probability of a standard normal random variable.
The theorem is stated below for completeness.</p>

<div class="theorem">
<span id="thm:berryesseen" class="theorem"><strong>Theorem 3.4  (Berry-Esseen)  </strong></span>Let <span class="math inline">\(Z_j\)</span>, <span class="math inline">\(j=1,\dots,n\)</span>, be independent random variables with mean <span class="math inline">\(\mu_j = \mathbb{E}[Z_j]\)</span>, variance <span class="math inline">\(\sigma_j^2 = \mathrm{Var}[Z_j]\)</span>, and third absolute central moment <span class="math inline">\(\theta_j = \mathbb{E}[|Z_j-\mu_j|^3]\)</span>.
Then, for every <span class="math inline">\(t \in\mathbb{R}\)</span>,
<span class="math display">\[ \left|\mathbb{P}\left[\frac{\sum_{j=1}^{n}\left(Z_j-\mu_j\right)}{\sqrt{\sum_{j=1}^{n}\sigma_j^2}} \geq t \right]-Q(t)\right| \leq \frac{6\sum_{j=1}^{n}\theta_j}{\left(\sum_{j=1}^{n}\sigma_j^2\right)^{3/2}}.  \]</span>
</div>

<p>A few observations about this theorem are in order:</p>
<ul>
<li><p>The error term is uniform in <span class="math inline">\(t\)</span>, i.e., it holds for all values of <span class="math inline">\(t\)</span>.</p></li>
<li><p>Assume that all <span class="math inline">\(Z_j\)</span> are identically distributed, and let <span class="math inline">\(\sigma_j=\sigma\)</span> and <span class="math inline">\(\theta_j=\theta\)</span>.
Then the error term in Therorem <a href="bi-awgn.html#thm:berryesseen">3.4</a> is given by <span class="math inline">\({6\theta}/(\sigma^3 \sqrt{n})\)</span>, i.e., it decays with <span class="math inline">\(n\)</span> as <span class="math inline">\(1/\sqrt{n}\)</span>.</p></li>
<li><p>The constant <span class="math inline">\(6\)</span> can actually be improved for the case in which the random variables are independent and identically distributed.</p></li>
</ul>
</div>
<div id="proof-asymptotic-expansion-of-the-rcus-bound" class="section level4">
<h4><span class="header-section-number">3.5.1.2</span> Proof: Asymptotic expansion of the RCUs bound</h4>
<p>In view of the application of Theorem <a href="bi-awgn.html#thm:berryesseen">3.4</a> to <a href="bi-awgn.html#eq:rcus-as-tail">(3.19)</a>, let <span class="math inline">\(T=\mathbb{E}\bigl[|\imath(X_1,Y_1)-C|^3\bigr]\)</span>.
Also, let <span class="math inline">\(\tilde{c}= \mathbb{E}[\log U]\)</span>, <span class="math inline">\(\tilde{v}=\mathrm{Var}[\log U]\)</span>, and <span class="math inline">\(\tilde{t}= \mathbb{E}\bigl[|\log U-\tilde{c}|^3\bigr]\)</span>.
Finally, let
<span class="math display">\[ B(n)= \frac{6(T+\tilde{t}/n)}{(V+\tilde{v}/n)^{3/2}}.\]</span>
It then follows from Theorem <a href="bi-awgn.html#thm:berryesseen">3.4</a> that
<span class="math display" id="eq:rcu-clt">\[\begin{align}
  \mathbb{P}\left[\sum_{j=1}^n \imath_s(X_j,Y_j) + \log U \leq \log(2^k-1)\right]  
  &amp;\leq Q\biggl(-\frac{\log(2^k-1) -nC - \tilde{c}}{\sqrt{nV+\tilde{v}}}\biggr) + \frac{B(n)}{\sqrt{n}} \\
  &amp;\leq Q\biggl(\frac{C-R +\tilde{c}/n}{\sqrt{(V+\tilde{v}/n)/n}}\biggr) + \frac{B(n)}{\sqrt{n}}.
  \tag{3.22}
\end{align}\]</span></p>
<p>Here, we used that <span class="math inline">\(Q(-x)=1-Q(x)\)</span> and the second inequality follows because <span class="math inline">\(\log (2^k -1)/n \leq R\)</span>.
To obtain the desired result, we set the right-hand side of <a href="bi-awgn.html#eq:rcu-clt">(3.22)</a> equal to <span class="math inline">\(\epsilon\)</span> and then we solve for <span class="math inline">\(R\)</span>.
By doing so, we conclude that the following rate <span class="math inline">\(R\)</span> is achievable for the chosen <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(n\)</span>:</p>
<p><span class="math display" id="eq:normal-rcus-1">\[\begin{align}
  R= C + \frac{\tilde{c}}{n}  -\sqrt{\frac{V+\tilde{v}/n}{n}}
  Q^{-1}\biggl(\epsilon-\frac{B(n)}{\sqrt{n}}\biggr)\\
  = C - \sqrt{\frac{V}{n}}Q^{-1}(\epsilon) + \mathcal{O}\left(\frac{1}{n}\right).
  \tag{3.23}
\end{align}\]</span>
In the last step, we performed some Taylor expansions and gathered in <span class="math inline">\(\mathcal{O}\left({1}/{n}\right)\)</span> all terms of order <span class="math inline">\(1/n\)</span>.</p>
</div>
<div id="proof-asymptotic-expansion-of-the-metaconverse-bound-verdú-han-relaxation" class="section level4">
<h4><span class="header-section-number">3.5.1.3</span> Proof: Asymptotic expansion of the metaconverse bound (Verdú-Han relaxation)</h4>
<p>We next establish a similar expansion for <a href="bi-awgn.html#eq:vh-bound-bi-awgn">(3.17)</a>.
Let <span class="math inline">\(B=6T/V^{3/2}\)</span>.
Instead of optimizing over <span class="math inline">\(\gamma\)</span> in <a href="bi-awgn.html#eq:vh-bound-bi-awgn">(3.17)</a>, we set
<span class="math display">\[ \gamma = nC - \sqrt{nV}Q^{-1}\left(\epsilon + \frac{2B}{\sqrt{n}}\right). \]</span></p>
<p>It then follows from Theorem <a href="bi-awgn.html#thm:berryesseen">3.4</a> that</p>
<p><span class="math display">\[ P_{Y^n | X^n=\bar{x}^n}\left[ \sum_{j=1}^n\imath(\bar{x}_j,Y_j) \geq \gamma \right] \leq Q\left(-Q^{-1}\left(\epsilon+\frac{2B}{\sqrt{n}}\right)\right) + \frac{B}{\sqrt{n}} = 1 - \epsilon - \frac{B}{\sqrt{n}}. \]</span>
Substituting this result into <a href="bi-awgn.html#eq:vh-bound-bi-awgn">(3.17)</a>, we conclude that every code with blocklength <span class="math inline">\(n\)</span> and error probability no larger than <span class="math inline">\(\epsilon\)</span>, must have a rate <span class="math inline">\(R\)</span> that satisfies</p>
<p><span class="math display" id="eq:metaconverse-na-1">\[\begin{align}
  R &amp;\leq C -\sqrt\frac{V}{n}Q^{-1}\left(\epsilon + \frac{2B}{\sqrt{n}}\right) -\frac{1}{n} \log \left(\frac{B}{\sqrt{n}}\right)\\
&amp;= C -\sqrt\frac{V}{n}Q^{-1}\left(\epsilon\right) +\frac{1}{2n} \log n + \mathcal{O}\left(\frac{1}{n}\right).
  \tag{3.24}
\end{align}\]</span>
In the last step, we performed a Taylor expansion of the <span class="math inline">\(Q^{-1}(\cdot)\)</span> function, and gathered in <span class="math inline">\(\mathcal{O}\left({1}/{n}\right)\)</span> all terms of order <span class="math inline">\(1/n\)</span>.</p>
</div>
<div id="proof-final-step" class="section level4">
<h4><span class="header-section-number">3.5.1.4</span> Proof: final step</h4>
<p>The desired result <a href="bi-awgn.html#eq:normal-approx-bi-awgn">(3.21)</a> follows simply by combining <a href="bi-awgn.html#eq:normal-rcus-1">(3.23)</a> with <a href="bi-awgn.html#eq:metaconverse-na-1">(3.24)</a>.</p>
<p>Note indeed that, in <a href="bi-awgn.html#eq:metaconverse-na-1">(3.24)</a>, we have that <span class="math inline">\(\frac{1}{2n} \log n + \mathcal{O}\left(\frac{1}{n}\right)= \mathcal{O}\left(\frac{\log n}{n}\right)\)</span>.
Furthermore, in <a href="bi-awgn.html#eq:normal-rcus-1">(3.23)</a>, we have that <span class="math inline">\(\mathcal{O}\left(\frac{1}{n}\right)\)</span> is also <span class="math inline">\(\mathcal{O}\left(\frac{\log n}{n}\right)\)</span>.</p>
</div>
</div>
<div id="the-accuracy-of-the-normal-approximation" class="section level3">
<h3><span class="header-section-number">3.5.2</span> The accuracy of the normal approximation</h3>
<p>It follows from <a href="bi-awgn.html#eq:normal-approx-bi-awgn">(3.21)</a> that the maximum coding rate <span class="math inline">\(R^\star(n,\epsilon)\)</span> can be approximated as</p>
<p><span class="math display" id="eq:normal-approx">\[\begin{equation}
  R^\star(n,\epsilon) \approx C -\sqrt\frac{V}{n}Q^{-1}(\epsilon).
  \tag{3.25}
\end{equation}\]</span></p>
<p>Computing the normal approximation <a href="bi-awgn.html#eq:normal-approx">(3.25)</a> is much simpler than computing the two nonasymptotic bounds we have introduced earlier. It takes only a few lines of matlab code:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode matlab"><code class="sourceCode matlab"><span id="cb4-1"><a href="bi-awgn.html#cb4-1"></a>Zmin = -<span class="fl">9</span>; Zmax = <span class="fl">9</span>;</span>
<span id="cb4-2"><a href="bi-awgn.html#cb4-2"></a>fC = @(z) exp(-z.^<span class="fl">2</span>/<span class="fl">2</span>)/sqrt(<span class="fl">2</span>*pi) .* (log(<span class="fl">2</span>) - log(<span class="fl">1</span>+exp(-<span class="fl">2</span>*rho-<span class="fl">2</span>*sqrt(rho)*z)));</span>
<span id="cb4-3"><a href="bi-awgn.html#cb4-3"></a>C = quadl(fC, Zmin, Zmax);</span>
<span id="cb4-4"><a href="bi-awgn.html#cb4-4"></a>fV = @(z) exp(-z.^<span class="fl">2</span>/<span class="fl">2</span>)/sqrt(<span class="fl">2</span>*pi) .* (log(<span class="fl">2</span>) - log(<span class="fl">1</span>+exp(-<span class="fl">2</span>*rho-<span class="fl">2</span>*sqrt(rho)*z))).^<span class="fl">2</span>;</span>
<span id="cb4-5"><a href="bi-awgn.html#cb4-5"></a>V = integral(fV, Zmin, Zmax) - (C)^<span class="fl">2</span>;</span>
<span id="cb4-6"><a href="bi-awgn.html#cb4-6"></a>R_NA = C - sqrt(V/n)*qfuncinv(epsilon);</span></code></pre></div>
<p>This makes the use of this approximation attractive whenever a finite-blocklength characterization of the transmission channel is required as part of optimization algorithms, such as the ones employed in wireless networks for power allocation and user scheduling.</p>
<p>Differently from the crude first-order approximation <span class="math inline">\(R^\star(n,\epsilon) \approx C\)</span>, the second-order approximation given in <a href="bi-awgn.html#eq:normal-approx">(3.25)</a> captures the penalty from operating at finite blocklength, which is of order <span class="math inline">\(1/\sqrt{n}\)</span>, via the channel dispersion <span class="math inline">\(V\)</span>.
It also captures the dependence on the target error probability via the multiplicative term <span class="math inline">\(Q^{-1}(\epsilon)\)</span>.</p>
<p>Often, a more accurate approximation is obtained by adding to the right-hand side of <a href="bi-awgn.html#eq:normal-approx">(3.25)</a> the term <span class="math inline">\((\log n)/(2n)\)</span> that appears in the asymptotic expansion of the converse bound <a href="bi-awgn.html#eq:metaconverse-na-1">(3.24)</a>.
We shall refer to the corresponding normal approximation
<span class="math display" id="eq:refined-normal-approx">\[\begin{equation}
  R^\star(n,\epsilon) \approx C -\sqrt\frac{V}{n}Q^{-1}(\epsilon) + \frac{1}{2n}\log(n).
  \tag{3.26}
\end{equation}\]</span>
as <strong>refined normal</strong> approximation.</p>
<p>Note that this approximation can be equivalently stated in terms of minimum error probability <span class="math inline">\(\epsilon^*(R,n)\)</span>:</p>
<p><span class="math display" id="eq:refined-normal-approx-error">\[\begin{equation}
  \epsilon^*(R,n)\approx Q\left( \frac{C-R +(\log n)/(2n)}{\sqrt{V/n}}\right).
  \tag{3.27}
\end{equation}\]</span>
Please note that the rate <span class="math inline">\(R\)</span> in <a href="bi-awgn.html#eq:refined-normal-approx-error">(3.27)</a> is measured in nats per channel use and not in bits per channel use.</p>
<p>The accuracy of the normal approximations just introduced is demonstrated in Figure <a href="bi-awgn.html#fig:nasnrvseps">3.6</a>, where we consider the same setup as in Figure <a href="bi-awgn.html#fig:epsvssnr">3.5</a>: <span class="math inline">\(n=128\)</span> and <span class="math inline">\(R=0.5\)</span> bit per channel use.
As shown in the figure, the normal approximation is fairly accurate over all range of SNR values considered in the figure.
The refined normal approximation is even more accurate and lies between the tightest achievability and converse bounds available for this setup.</p>
<div class="figure" style="text-align: center"><span id="fig:nasnrvseps"></span>
<img src="images/Section3/figure3_6/snr_vs_eps_NA.png" alt="Normal approximation vs. finite-blocklength upper and lower bounds on $\epsilon^*(n,R)$ as a function of the SNR (dB) for $R=1/2$ bits per channel use and $n=128$." width="70%" />
<p class="caption">
Figure 3.6: Normal approximation vs. finite-blocklength upper and lower bounds on <span class="math inline">\(\epsilon^*(n,R)\)</span> as a function of the SNR (dB) for <span class="math inline">\(R=1/2\)</span> bits per channel use and <span class="math inline">\(n=128\)</span>.
</p>
</div>
<p>So, at least for the scenario considered in Fig. <a href="bi-awgn.html#fig:nasnrvseps">3.6</a>, the normal approximation is accurate.</p>
<p>When can we expect this to be the case? Roughly speaking, when deriving <a href="bi-awgn.html#eq:normal-approx-bi-awgn">(3.21)</a>, we approximated
<span class="math display">\[ Q\left(\frac{C-R}{\sqrt{V/n}}\right) + \frac{\mathrm{c}}{\sqrt{n}} = \epsilon \]</span></p>
<p>where <span class="math inline">\(\mathrm{c}\)</span> is a constant, with
<span class="math display">\[ Q\left(\frac{C-R}{\sqrt{V/n}}\right) \approx \epsilon. \]</span></p>
<p>This approximation is obviously inaccurate when <span class="math inline">\(Q\bigl((C-R)/\sqrt{V/n}\bigr) \ll c/\sqrt{n}\)</span>.
This may occur if <span class="math inline">\(R\ll C\)</span> in which case the <span class="math inline">\(Q(\cdot)\)</span> may take very small value.
It may also occur when <span class="math inline">\(n\)</span> is small.
We illustrate this issue in Figure <a href="bi-awgn.html#fig:nafblawgnevr">3.7</a> below.</p>
<div class="figure" style="text-align: center"><span id="fig:nafblawgnevr"></span>
<img src="images/Section3/figure3_7/eps_vs_rate_withNA.png" alt="Normal approximation vs finite-blocklength upper and lower bounds on $\epsilon^*(n,R)$ as a function of the rate $R$ for different values of the blocklength $n$" width="70%" />
<p class="caption">
Figure 3.7: Normal approximation vs finite-blocklength upper and lower bounds on <span class="math inline">\(\epsilon^*(n,R)\)</span> as a function of the rate <span class="math inline">\(R\)</span> for different values of the blocklength <span class="math inline">\(n\)</span>
</p>
</div>
<p>Here, we assume <span class="math inline">\(\rho=0.189\)</span> dB, for which <span class="math inline">\(C=0.5\)</span> bits per channel use, and evaluate the tightest achievability and converse bounds available together with the refined normal approximation for different values of the rate <span class="math inline">\(R\)</span> measured in bits per second.
As shown in the figure, the normal approximation loses accuracy as the gap between <span class="math inline">\(R\)</span> and <span class="math inline">\(C\)</span> increases and this is particularly problematic for small blocklengths.</p>
<p>To summarize, the normal approximation may be inaccurate when the packets are short, and the selected rate is significantly smaller than the channel capacity, a choice that is unavoidable if one wants to achieve low error probabilities at low SNR values.
This is a scenario that is relevant for the ultra-reliable low-latency communication (URLLC) use case recently introduced in 5G.
Hence, we conclude that for the URLLC scenario, the use of the normal approximation to predict system performance and perform system design may be questionable.</p>
</div>
</div>
<div id="performance-of-practical-error-correcting-codes" class="section level2">
<h2><span class="header-section-number">3.6</span> Performance of practical error-correcting codes</h2>
<p>How close do actual error-correcting codes perform to the bounds just discussed?
This questions has been analyzed in many recent papers in the literature.
Here, we present some of the results reported in <span class="citation">[<a href="#ref-coskun19-03a" role="doc-biblioref">11</a>]</span>.
Please consult this reference for more details about the families of codes discussed here as well as for additional results pertaining some of the codes selected in 5G.</p>
<div class="figure" style="text-align: center"><span id="fig:codes64-128"></span>
<img src="images/Section3/figure3_8-3_9/case64.png" alt="Error probability vs. SNR in dB for selected families of channel codes. In the figure, $k=64$ and $n=128$" width="100%" />
<p class="caption">
Figure 3.8: Error probability vs. SNR in dB for selected families of channel codes. In the figure, <span class="math inline">\(k=64\)</span> and <span class="math inline">\(n=128\)</span>
</p>
</div>
<p>In Figure <a href="bi-awgn.html#fig:codes64-128">3.8</a>, we plot the RCU and metaconverse bounds for the case <span class="math inline">\(k=64\)</span> and <span class="math inline">\(n=128\)</span> as well as the performance of selected classes of codes.
As shown in the figure, tail-biting convolutional codes (TBCC) of sufficiently large memory (<span class="math inline">\(m=14\)</span>) perform very close to the bounds.
Modern short codes such as turbo codes and low-density parity-check (LDPC) codes perform worse at these very short blocklengths.
In the figure, we reported the performance of the binary LDPC code selected by the Consultative Committee for Space Data Systems (CCSDS) as error-correcting code for the satellite command,
as well as the ones of an accumulate-repeat-accumulate (ARA) LDPC code, and a turbo code based on 16-state component recursive convolutional codes.
As shown in the figure, better performance (at the cost of an increased decoding complexity) can be obtained with a nonbinary LDPC code over <span class="math inline">\(\mathbb{F}_{256}\)</span>.
Finally, one can achieve performance similar to the ones of TBCCs by using a polar code with successive-cancellation list-decoding (with list size <span class="math inline">\(L=32\)</span>) combined with an outer CRC code of length <span class="math inline">\(7\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:codes256-512"></span>
<img src="images/Section3/figure3_8-3_9/case256.png" alt="Error probability vs. SNR in dB for selected families of channel codes. In the figure, $k=256$ and $n=512$" width="100%" />
<p class="caption">
Figure 3.9: Error probability vs. SNR in dB for selected families of channel codes. In the figure, <span class="math inline">\(k=256\)</span> and <span class="math inline">\(n=512\)</span>
</p>
</div>
<p>In Figure <a href="bi-awgn.html#fig:codes256-512">3.9</a>, we consider the case <span class="math inline">\(k=256\)</span> and <span class="math inline">\(n=512\)</span>.
For this scenario, the performance of the TBCCs are not as satisfactory. Indeed, as expected, without further increasing the memory of the constituent convolutional codes, the performance of TBCCs degrade as the blocklength increases.
On the contrary, modern short codes have better performance, which is again expected. The best performance in the figure is achieved by a polar code with list-cancellation decoding and list size <span class="math inline">\(L=1024\)</span>, which exhibits a gap of around <span class="math inline">\(0.5\)</span> dB to the nonasymptotic bounds when <span class="math inline">\(\epsilon=10^{-4}\)</span>.</p>
<!--
<span style="color:red">[To be written]</span> -->

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-polyanskiy10-05a">
<p>[1] Y. Polyanskiy, H. V. Poor, and S. Verdú, “Channel coding rate in the finite blocklength regime,” <em>IEEE Trans. Inf. Theory</em>, vol. 56, no. 5, pp. 2307–2359, May 2010.</p>
</div>
<div id="ref-shannon48-07a">
<p>[3] C. E. Shannon, “A mathematical theory of communication,” <em>Bell Syst. Tech. J.</em>, vol. 27, pp. 379–423 and 623–656, 1948.</p>
</div>
<div id="ref-yang18-09a">
<p>[4] W. Yang, A. Collins, G. Durisi, Y. Polyanskiy, and H. V. Poor, “Beta-beta bounds: Finite-blocklength analog of the golden formula,” <em>IEEE Trans. Inf. Theory</em>, vol. 64, no. 9, pp. 6236–6256, Sep. 2018.</p>
</div>
<div id="ref-martinez11-02a">
<p>[5] A. Martinez and A. Guillén i Fàbregas, “Saddlepoint approximation of random–coding bounds.” San Diego, CA, U.S.A., Feb. 2011.</p>
</div>
<div id="ref-gallager68a">
<p>[6] R. G. Gallager, <em>Information theory and reliable communication</em>. New York, NY, U.S.A.: Wiley, 1968.</p>
</div>
<div id="ref-font-segura18-03a">
<p>[7] J. Font-Segura, G. Vazquez-Vilar, A. Martinez, A. Guillén i Fàbregas, and A. Lancho, “Saddlepoint approximations of lower and upper bounds to the error probability in channel coding.” Princeton, NJ, Mar. 2018.</p>
</div>
<div id="ref-polyanskiy19-05a">
<p>[8] Y. Polyanskiy and Y. Wu, <em>Lecture notes on information theory</em>. 2019.</p>
</div>
<div id="ref-verdu94-07a">
<p>[9] S. Verdú and T. S. Han, “A general formula for channel capacity,” <em>IEEE Trans. Inf. Theory</em>, vol. 40, no. 4, pp. 1147–1157, Jul. 1994.</p>
</div>
<div id="ref-feller71-a">
<p>[10] W. Feller, <em>An introduction to probability theory and its applications</em>, vol. II. New York, NY, USA: Wiley, 1971.</p>
</div>
<div id="ref-coskun19-03a">
<p>[11] M. C. Coşkun <em>et al.</em>, “Efficient error-correcting codes in the short blocklength regime,” <em>Physical Communication</em>, Mar. 2019.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["fbl-book.pdf", "fbl-book.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
